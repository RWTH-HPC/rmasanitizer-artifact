
\documentclass[twoside]{article}

\usepackage{url}

\begin{document}
\section*{Abstract Identification}
%START_LATEX
This is the artifact description for the paper ``RMASanitizer: Generalized Runtime Detection of Data Races in RMA Applications'' submitted to the ICPP'24 conference.


\subsection*{Main Contributions}
The paper corresponding to this artifact contributes a generalized race detection tool for Remote Memory Access Programs. In the paper, we make the following contributions: (1) We provide a generalized RMA race detection model compatible with state-of-the-art RMA programming models. (2) We present RMASanitizer, an on-the-fly race detector that works with MPI RMA, OpenSHMEM, and GASPI. (3) We evaluate the classification quality of RMASanitizer and present overhead studies on different RMA proxy apps.

\subsection*{Role of the Artifact}
The artifact provides the source code of RMASanitizer itself to review and understand the software architecture described in the paper. Further, it provides all raw data that was gathered during the experiments to generate the classification quality benchmark (Table 3 of the paper) and overhead study results (Figure 9 of the paper). Also, the artifact provides all resources to reproduce the results of the classification quality benchmark (Table 3 of the paper) as well as the overhead study results (Figure 9 of the paper).


\subsection*{Computational Artifact Structure}
The computational artifact contains the following folders:
\begin{itemize}
    \item \texttt{RMASanitizer}: Source code of RMASanitizer (for a detailed explanation, see the README in the computational artifact) (Section 5)
    \item \texttt{classification\_quality}: Results of RMASanitizer, MUST-RMA and PARCOACH-(dynamic,static) on RMARaceBench (Table 3, Section 6.1)
    \item \texttt{overhead\_evaluation}: Results of RMASanitizer and MUST-RMA on the different proxy apps considered in the paper (Figure 9, Section 6.2)
    \item \texttt{overhead\_evaluation/plots}: Resulting plots (Figure 9, Section 6.2)
    \item \texttt{classification\_quality.sh}: Script to reproduce classification quality results (Table 3, Section 6.1)
    \item \texttt{overhead\_submit.sh/overhead\_results.sh}: Scripts to reproduce the overhead results (Figure 9, Section 6.2)
\end{itemize}


\subsection*{Classification Quality Results}
We ran each tool on the RMARaceBench suite. 
In the computational artifact, the results can be found in the folder \texttt{classification\_quality}.
Each output folder also includes the parsed output from the RMARaceBench evaluation script, \texttt{results\_parsed}.
Those result values were copied into Table 3.

For reproducibility, the required software is shipped in a Docker container (see \texttt{classification\_quality/Dockerfile}). The following software versions are used in the container:
\begin{itemize}
\item Debian 12
\item OpenMPI 4.1.6 for MPI RMA, Sandia SHMEM 1.5.2 for OpenSHMEM, GPI-2 1.5.1 for GASPI
\item Clang 16
\item RMASanitizer (provided in the artifact), MUST-RMA\footnote{\url{https://hpc.rwth-aachen.de/must/files/MUST-v1.9.0-rma.tar.gz}} (provided in the artifact), PARCOACH\footnote{\url{https://gitlab.inria.fr/parcoach/parcoach/-/archive/2.4.2/parcoach-2.4.2.tar.gz}}
\end{itemize}


\subsection*{Overhead Study Results}
The performance evaluation uses the JUBE benchmarking environment\footnote{\url{https://apps.fz-juelich.de/jsc/jube/jube2/docu}} for reproducible benchmark setups.
The raw performance results are available in the folder \texttt{overhead\_evaluation} for RMASanitizer and MUST-RMA in subfolders for each benchmark. In particular, each subfolder has a folder  \texttt{result} that contains the measured results in a CSV file \texttt{result\_csv.dat} and in a human-readable \texttt{result.dat} format.

The following software versions were used to run the experiments:
\begin{itemize}
\item Rocky Linux 8
\item OpenMPI 4.1.6 for MPI RMA, Sandia SHMEM 1.5.2 for OpenSHMEM, GPI-2 1.5.1 for GASPI
\item Clang / Classic-Flang 16
\item RMASanitizer (provided in artifact), MUST-RMA\footnote{\url{https://hpc.rwth-aachen.de/must/files/MUST-v1.9.0-rma.tar.gz}} (provided in artifact)
\item JUBE 2.6.1
\item Slurm scheduler to schedule the jobs generated with JUBE
\item Python 3 with pandas, matplotlib, seaborn (for plots)
\end{itemize}

%STOP_LATEX

\section*{Reproducibility of Experiments}

%START_LATEX
In the following, we document the steps to reproduce the results of the classification quality and the overhead study.
For the classification quality experiment, we expect to set up and run everything within 2 hours (the test cases take some time to get executed).
For the overhead study, in case of all software requirements installed, the execution requires running a few JUBE commands and finally parsing the output with a Python script. While running all the commands will only take a few minutes, the execution of the jobs may take a few hours.

\subsection*{Classification Quality}
To reproduce the classification quality results, an installation of Docker is required.
All required step can be executed by running the script \texttt{classification\_quality.sh} of the computational artifact.
The workflow first creates the Docker container of RMARaceBench using a Dockerfile.
The container itself comes with a shell script that runs all commands to reproduce the results.
Finally, the results are copied out of the container and additionally printed to the command line.

The outputs and results generated by the script should match those given in the computational artifact and represent the Table 3 in the paper.
In particular, the files \texttt{results\_parsed} in each result folder contain the values (TP, TN, FP, FN, Accuracy) that have been copied into the table.

Note: In very recent Linux kernel versions newer than 6.0, the address space layout randomization (ASLR) bit count must be below or equal 28. This is a requirement of the ThreadSanitizer version shipped MUST-RMA and RMASanitizer. This can be done temporarily (until system reboot) on typical Linux systemd-based systems using \texttt{sysctl vm.mmap\_rnd\_bits=28}. The command needs root permissions. 


\subsection*{Overhead Study}
The overhead study is designed in JUBE.
For the results in our paper, we used OpenMPI, Sandia SHMEM, GPI-2, Clang/Classic-Flang.
For MUST-RMA, the executed benchmarks are \texttt{PRK\_stencil}, \texttt{BT-RMA}, \texttt{miniMD}, \texttt{lulesh}.
For RMASanitizer, the executed benchmarks are \texttt{PRK\_stencil}, \texttt{BT-RMA},  \texttt{miniMD}, \texttt{lulesh} \texttt{PRK\_stencil\_shmem}, \texttt{BT-SHMEM}, \texttt{CFD-Proxy}.

All steps for reproduction are available in a script \texttt{overhead\_submit.sh} and \texttt{overhead\_results.sh} that can be executed to generate the results and the final plot.
Using \texttt{overhead\_submit.sh}, a JUBE workflow automatically builds the tools (RMASanitizer and MUST-RMA), builds the source codes, submits the jobs with the correct input sizes, and finally evaluates the results to generate output tables.
After all jobs completed, the results can be retrieved by running \texttt{overhead\_results.sh} that parses the outputs of all jobs to get the results and invokes a plotting script.
This will generate exactly the plots of Figure 9 in the paper.
The paper also makes statements about the memory usage and the number of RMA operations executed per second.
This data is also included in the result tables generated by JUBE that can be found in the \texttt{result} folder of each benchmark.

%STOP_LATEX


\section*{Artifact Dependencies and Requirements}

%START_LATEX
The classification quality benchmarks can be executed on any arbitrary system with Docker installed.
The overhead benchmarks were executed on the CLAIX-2023 cluster\footnote{\url{https://help.itc.rwth-aachen.de/service/rhr4fjjutttf/article/fbd107191cf14c4b8307f44f545cf68a/}} of RWTH Aachen University. Each node is equipped with 2x Intel Xeon 8468 Sapphire Rapids with disabled SMT and 256 GB of main memory. The nodes are connected via InfiniBand. As the overhead benchmarks run experiments with up to 768 processes (requiring up to 1536 cores), any cluster with exactly or more than 1536 cores is feasible to run the experiments.

Different to the experiments in the paper, the overhead benchmarks can also run on a single compute node within Docker for simple reproducibility (small-scale experiments).
For the small-scale experiments, we assume that a compute node with 256 cores is used such that tests with up to 128 application processes can be executed.

There are no specific restrictions on the required operating system since everything can run in a Docker container. The required software libraries for reproducibility are listed in the abstract identification.
The input data sets for the original overhead experiment in the paper have been chosen to represent realistic workloads.
For the small-scale experiments, the number of iterations and region sizes have been reduced.
Qualitatively, the results of the small-scale experiment are similar to that of the large-scale experiment in the paper: RMASanitizer has a significant smaller slowdown than MUST-RMA. With an increasing number of processes, the slowdown of RMASanitizer (and also MUST-RMA) increases.
For details on the concrete changes, we refer to the computational artifact.
All used datasets are publicly available and shipped with the computational artifact.

%STOP_LATEX

\section*{Artifact Installation Deployment Process}

%START_LATEX

For simplified execution of the experiments, we provide a ChameleonCloud script that sets up a machine suitable for running all evaluations.
The script is part of the computational artifact and can be executed with
{
\small
\begin{verbatim}
$ ./reserve_chameleon_node.sh
\end{verbatim}
}
It will start up a properly configured node (\texttt{compute\_zen3}) using using the infrastructure of CHI@TACC with \texttt{openstack}. The image used for the nodes is \texttt{ubuntu2204-rmasan} (CHI@TACC).
If \texttt{openstack} is not installed on the executing (host) system, a corresponding Docker image can be built and executed with
{
\small
\begin{verbatim}
$ ./start_openstack_image.sh
\end{verbatim}
}
After reserving the node, the script automatically connects via SSH to the machine.
On the machine, the following script available on the machine itself downloads the artifact to the node:
{
\small
\begin{verbatim}
$ ./bootstrap.sh
\end{verbatim}
}

Within the artifact folder, the classification quality benchmark (expected execution time: 2 hours) can be executed with
{
\small
\begin{verbatim}
$ ./classification_quality.sh
\end{verbatim}
}

The results will be available in the folder \texttt{cq-results-YYMMDD-HHMMSS} for further investigation. The files in the \texttt{summaries} folder can be used to compare the reproduced results with the reference results (see \texttt{classification\_quality} folder).
The script will also print out the summarized results to the command line.

The small-scale experiments of the overhead benchmarks (expected execution time: 4 hours) can be executed with
{
\small
\begin{verbatim}
$ ./overhead_evaluation_chameleon.sh
\end{verbatim}
}

The results will be available in the folder \texttt{perf-results-YYMMDD-HHMMSS}.
In particular, the resulting plotted PNG file is contained in the folder.
For reference, the paper results (large-scale experiment) and the Chameleon cloud results (small-scale experiment) are in the folder \texttt{overhead\_evaluation/plots/}.

To access the result files from outside container, the script 

{
\small
\begin{verbatim}
$ ./copy_results_to_objectstorage.sh
\end{verbatim}
}

copies the result to the ChameleonCloud object storage.
The files can then be viewed on the ChameleonCloud object storage file viewer.

%STOP_LATEX

\end{document}