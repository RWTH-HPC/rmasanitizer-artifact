
\documentclass[twoside]{article}

\usepackage{url}

\begin{document}
This is the artifact description for the paper ``RMASanitizer: Generalized Runtime Detection of Data Races in RMA Applications'' submitted to the ICPP'24 conference.

\subsection*{Main Contributions}
The paper corresponding to this artifact contributes a generalized race detection tool for Remote Memory Access Programs. In the paper, we make the following contributions: (1) We provide a generalized RMA race detection model compatible with state-of-the-art RMA programming models. (2) We present RMASanitizer, an on-the-fly race detector that works with MPI RMA, OpenSHMEM, and GASPI. (3) We evaluate the classification quality of RMASanitizer and present overhead studies on different RMA proxy apps.

\subsection*{Role of the Artifact}
The artifact provides the source code of RMASanitizer itself to review and understand the software architecture of RMASanitizer described in the paper. Further, it provides all raw data that was gathered during the experiments to generate the classification quality benchmark (Table 3 of the paper) and overhead study results (Figure 9 of the paper). Also, the artifact provides all resources to reproduce the results of the classification quality benchmark (Table 3 of the paper) as well as the overhead study results (Figure 9 of the paper).


\subsection*{Computational Artifact Structure}
The computational artifact is available at 
\begin{center}
\url{https://hpc.rwth-aachen.de/papers/icpp24-rmasanitizer.tar.gz}.
\end{center}

It will be published on Zenodo and GitHub upon paper acceptance.
The computational artifact contains the following folders:
\begin{itemize}
    \item \texttt{RMASanitizer}: Source code of RMASanitizer (for a detailed explanation, see the README in the computational artifact) (Section 5)
    \item \texttt{classification\_quality}: Results of RMASanitizer, MUST-RMA and PARCOACH-(dynamic,static) on RMARaceBench (Table 3, Section 6.1)
    \item \texttt{evaluation}: Results of RMASanitizer and MUST-RMA on the different proxy apps considered in the paper (Figure 9, Section 6.2)
    \item \texttt{classification\_quality.sh}: Script to reproduce classification quality results (Table 3, Section 6.1)
    \item \texttt{overhead\_submit.sh/overhead\_results.sh}: Scripts to reproduce the overhead results (Figure 9, Section 6.2)
\end{itemize}


\subsection*{Classification Quality Results}
We ran each tool on the RMARaceBench suite. 
In the computational artifact, the results can be found in the folder \texttt{classification\_quality}.
Each output folder also includes the parsed output from the RMARaceBench evaluation script, \texttt{results\_parsed}.
Those result values were copied into Table 3.

For reproducibility, the required software is shipped in a Docker container (see \texttt{classification\_quality/Dockerfile}). The following software versions are used in the container:
\begin{itemize}
\item Debian 12
\item OpenMPI 4.1.6 for MPI RMA, Sandia SHMEM 1.5.2 for OpenSHMEM, GPI-2 1.5.1 for GASPI
\item Clang 16
\item RMASanitizer (provided in the computational artifact), MUST-RMA\footnote{\url{https://hpc.rwth-aachen.de/must/files/MUST-v1.9.0-rma.tar.gz}}, PARCOACH\footnote{\url{https://gitlab.inria.fr/parcoach/parcoach/-/archive/2.3.1/parcoach-2.3.1.tar.gz}}
\end{itemize}

The classification quality benchmarks can be executed on any arbitrary system with Docker or Podman installed.


\subsection*{Overhead Study Results}
The performance evaluation uses the JUBE benchmarking environment\footnote{\url{https://apps.fz-juelich.de/jsc/jube/jube2/docu}} for reproducible benchmark setups.
The raw performance results are available in the folder \texttt{evaluation/} for RMASanitizer and MUST-RMA in subfolders for each benchmark. In particular, each subfolder has a folder  \texttt{result} that contains the measured results in a CSV file \texttt{result\_csv.dat} and in a human-readable \texttt{result.dat} format.

The following software versions were used to run the experiments:
\begin{itemize}
\item Rocky Linux 8
\item OpenMPI 4.1.6 for MPI RMA, Sandia SHMEM 1.5.2 for OpenSHMEM, GPI-2 1.5.1 for GASPI
\item Clang / Classic-Flang 16
\item RMASanitizer (provided in the computational artifact), MUST-RMA\footnote{\url{https://hpc.rwth-aachen.de/must/files/MUST-v1.9.0-rma.tar.gz}}
\item JUBE 2.6.1
\item Slurm scheduler to schedule the jobs generated with JUBE
\item Python 3 with pandas, matplotlib, seaborn (for plots)
\end{itemize}

The overhead benchmarks were executed on the CLAIX-23 cluster\footnote{\url{https://help.itc.rwth-aachen.de/service/rhr4fjjutttf/article/fbd107191cf14c4b8307f44f545cf68a/}} of RWTH Aachen University. Each node is equipped with 2x Intel Xeon 8468 Sapphire Rapids with disabled SMT and 256 GB of main memory. The nodes are connected via InfiniBand.

As the overhead studies run experiments with up to 768 processes (requiring up to 1536 cores), any cluster with up to 1536 cores and preferably an InifiniBand interconnect should be feasible to run the experiments. Upon request, we can also grant access to CLAIX-23 to reproduce our measurements.

\end{document}