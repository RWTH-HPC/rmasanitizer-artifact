Using prebuild /home/user/.cache/must/prebuilds/47db29af17fd8ef1795e84c042421780
Using prebuild /home/user/.cache/must/prebuilds/47db29af17fd8ef1795e84c042421780
[MUST] MUST configuration ... centralized checks with fall-back application crash handling (very slow)
[MUST] Information: overwritting old intermediate data in directory "/rmaracebench/must_temp"!
[MUST] Using prebuilt infrastructure at /home/user/.cache/must/prebuilds/47db29af17fd8ef1795e84c042421780
[MUST] Weaver ... success
[MUST] Generating P^nMPI configuration ... success
[MUST] Infrastructure in "/home/user/.cache/must/prebuilds/47db29af17fd8ef1795e84c042421780" is present and used.
[MUST] Search for linked P^nMPI ... not found ... using LD_PRELOAD to load P^nMPI ... success
[MUST] Note: MPI RMA support requires the application to be built with Clang >= 15.Executing application:
[MUST-RUNTIME] [RMASanitize] epoch counter not found for context 0this should not happen.
[MUST-RUNTIME] [RMASanitize] epoch counter not found for context 0this should not happen.
Process 0: Execution finished, variable contents: localbuf[0] = 0, remote_data[0] = 0
Process 2: Execution finished, variable contents: localbuf[0] = 42, remote_data[0] = 0
Process 1: Execution finished, variable contents: localbuf[0] = 0, remote_data[0] = 42
[MUST-REPORT] Error: from: call @1: Remote data race at rank 1 between a write of size 4 at  Concurrent region of reference 1 started at reference 3 and ended at reference 4. 
[MUST-REPORT]  Concurrent region of reference 2 started at reference 5 and ended at reference 6. 
[MUST-REPORT]  References of a representative process:
[MUST-REPORT] Reference 1: call gaspi_writemain
[MUST-REPORT] /rmaracebench/results-20240409-164631/RMASanitizer/conflict/039-GASPI-conflict-read_notify-write-remote-yes.c:75
[MUST-REPORT]  @rank 2, threadid 1;
[MUST-REPORT] Reference 2: call gaspi_read_notifymain
[MUST-REPORT] /rmaracebench/results-20240409-164631/RMASanitizer/conflict/039-GASPI-conflict-read_notify-write-remote-yes.c:68
[MUST-REPORT]  @rank 0, threadid 1;
[MUST-REPORT] Reference 3: call gaspi_barriermain
[MUST-REPORT] /rmaracebench/results-20240409-164631/RMASanitizer/conflict/039-GASPI-conflict-read_notify-write-remote-yes.c:64
[MUST-REPORT]  @rank 1, threadid 0;
[MUST-REPORT] Reference 4: call  @rank 1, threadid 0;
[MUST-REPORT] Reference 5: call gaspi_barriermain
[MUST-REPORT] /rmaracebench/results-20240409-164631/RMASanitizer/conflict/039-GASPI-conflict-read_notify-write-remote-yes.c:64
[MUST-REPORT]  @rank 1, threadid 0;
[MUST-REPORT] Reference 6: call  @rank 1, threadid 0;
[MUST-REPORT] Information: from: call MPI_Finalize@0-2: The following MPI functions were used but are not supported by MUST: 
[MUST-REPORT] gaspi_proc_rank
[MUST-REPORT] gaspi_proc_term
[MUST-REPORT]  References of a representative process:
[MUST-REPORT] Reference 1: call gaspi_proc_rankmain
[MUST-REPORT] /rmaracebench/results-20240409-164631/RMASanitizer/conflict/039-GASPI-conflict-read_notify-write-remote-yes.c:39
[MUST-REPORT]  @rank 1, threadid 1;
[MUST-REPORT] Reference 2: call gaspi_proc_term??
[MUST-REPORT] ??:0
[MUST-REPORT]  @rank 1, threadid 1;
*** The MPI_Finalize() function was called after MPI_FINALIZE was invoked.
*** This is disallowed by the MPI standard.
*** Your MPI job will now abort.
[2d4db3254d3c:72028] Local abort after MPI_FINALIZE started completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
*** The MPI_Finalize() function was called after MPI_FINALIZE was invoked.
*** This is disallowed by the MPI standard.
*** Your MPI job will now abort.
[2d4db3254d3c:72030] Local abort after MPI_FINALIZE started completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
*** The MPI_Finalize() function was called after MPI_FINALIZE was invoked.
*** This is disallowed by the MPI standard.
*** Your MPI job will now abort.
[2d4db3254d3c:72029] Local abort after MPI_FINALIZE started completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[26181,1],0]
  Exit code:    1
--------------------------------------------------------------------------
[MUST] Execution finished.
