Using prebuild /home/user/.cache/must/prebuilds/3ceb8dba3ca5ad416757181e9fcd32f3
Using prebuild /home/user/.cache/must/prebuilds/3ceb8dba3ca5ad416757181e9fcd32f3
[MUST] MUST configuration ... centralized checks with fall-back application crash handling (very slow)
[MUST] Information: overwritting old intermediate data in directory "/rmaracebench/must_temp"!
[MUST] Using prebuilt infrastructure at /home/user/.cache/must/prebuilds/3ceb8dba3ca5ad416757181e9fcd32f3
[MUST] Infrastructure in "/home/user/.cache/must/prebuilds/3ceb8dba3ca5ad416757181e9fcd32f3" is present and used.
[MUST] Search for linked P^nMPI ... not found ... using LD_PRELOAD to load P^nMPI ... success
[MUST] Note: MPI RMA support requires the application to be built with Clang >= 15.Executing application:
Process 1: Execution finished, variable contents: value = 1, value2 = 2, win_base[0] = 0
Process 2: Execution finished, variable contents: value = 1, value2 = 2, win_base[0] = 42
Process 0: Execution finished, variable contents: value = 1, value2 = 2, win_base[0] = 0
[MUST-REPORT] Error: from: call @2: Remote data race at rank 2 between a read of size 4 at  Concurrent region of reference 1 started at reference 3 and ended at reference 4. 
[MUST-REPORT]  Concurrent region of reference 2 started at reference 5 and ended at reference 6. 
[MUST-REPORT]  References of a representative process:
[MUST-REPORT] Reference 1: call MPI_Getmain
[MUST-REPORT] /rmaracebench/results-20240330-171252/RMASanitizer/sync/035-MPI-sync-pscw-remote-yes.c:82
[MUST-REPORT]  @rank 1, threadid 1;
[MUST-REPORT] Reference 2: call MPI_Putmain
[MUST-REPORT] /rmaracebench/results-20240330-171252/RMASanitizer/sync/035-MPI-sync-pscw-remote-yes.c:73
[MUST-REPORT]  @rank 0, threadid 1;
[MUST-REPORT] Reference 3: call MPI_Win_postmain
[MUST-REPORT] /rmaracebench/results-20240330-171252/RMASanitizer/sync/035-MPI-sync-pscw-remote-yes.c:90
[MUST-REPORT]  @rank 2, threadid 0;
[MUST-REPORT] Reference 4: call  @rank 2, threadid 0;
[MUST-REPORT] Reference 5: call MPI_Win_postmain
[MUST-REPORT] /rmaracebench/results-20240330-171252/RMASanitizer/sync/035-MPI-sync-pscw-remote-yes.c:90
[MUST-REPORT]  @rank 2, threadid 0;
[MUST-REPORT] Reference 6: call  @rank 2, threadid 0;
[MUST-REPORT] Error: from: call MPI_Comm_group@0-1: There are 2 groups that are not freed when MPI_Finalize was issued, a quality application should free all MPI resources before calling MPI_Finalize. Listing information for these groups:
[MUST-REPORT] 
[MUST-REPORT]  -Group 1: Group created at reference  1 size=3
[MUST-REPORT] 
[MUST-REPORT]  -Group 2: Group created at reference  2 size=1 References of a representative process:
[MUST-REPORT] Reference 1: call MPI_Comm_groupmain
[MUST-REPORT] /rmaracebench/results-20240330-171252/RMASanitizer/sync/035-MPI-sync-pscw-remote-yes.c:61
[MUST-REPORT]  @rank 1, threadid 1;
[MUST-REPORT] Reference 2: call MPI_Group_inclmain
[MUST-REPORT] /rmaracebench/results-20240330-171252/RMASanitizer/sync/035-MPI-sync-pscw-remote-yes.c:78
[MUST-REPORT]  @rank 1, threadid 1;
[MUST-REPORT] Error: from: call MPI_Comm_group@2: There are 2 groups that are not freed when MPI_Finalize was issued, a quality application should free all MPI resources before calling MPI_Finalize. Listing information for these groups:
[MUST-REPORT] 
[MUST-REPORT]  -Group 1: Group created at reference  1 size=3
[MUST-REPORT] 
[MUST-REPORT]  -Group 2: Group created at reference  2 size=2 References of a representative process:
[MUST-REPORT] Reference 1: call MPI_Comm_groupmain
[MUST-REPORT] /rmaracebench/results-20240330-171252/RMASanitizer/sync/035-MPI-sync-pscw-remote-yes.c:61
[MUST-REPORT]  @rank 2, threadid 1;
[MUST-REPORT] Reference 2: call MPI_Group_inclmain
[MUST-REPORT] /rmaracebench/results-20240330-171252/RMASanitizer/sync/035-MPI-sync-pscw-remote-yes.c:88
[MUST-REPORT]  @rank 2, threadid 1;
[MUST] Execution finished.
